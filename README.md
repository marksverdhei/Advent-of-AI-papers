# Advent-of-AI-papers  

Papers are linked / listed below, but you can find their PDF versions in the `/pdfs/` directory in this repository.

All papers are discussed in the [NLP Norge discord server](https://discord.gg/cNn3HqRD) 

This repo serves as an advent calendar for research papers in AI and NLP

This is an advent calendar reading list for each day of Advent, 1 through 24.
I will add a paper, one day at a time. 

### Luke / Windows  

1. Are Emergent Abilities of Large Language Models a Mirage? [NeurIPS 2023 Paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html)

2. Scaling Laws for Neural Language Models [Arxiv](https://arxiv.org/abs/2001.08361)

3. A Watermark for Large Language Models [PMLR](https://proceedings.mlr.press/v202/kirchenbauer23a.html)

4. OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents [NeurIPS](https://nips.cc/virtual/2023/poster/73589)

5. The Surprising Effectiveness of Test-Time Training for Abstract Reasoning [Arxiv](https://arxiv.org/abs/2411.07279)

6. On the Measure of Intelligence [Arxiv](https://arxiv.org/abs/1911.01547)

7. Beyond neural scaling laws: beating power law scaling via data pruning [Arxiv](https://arxiv.org/abs/2206.14486)

8. Training Compute-Optimal Large Language Models [Arxiv](https://arxiv.org/abs/2203.15556)

9. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models [Arxiv](https://arxiv.org/abs/2410.05229v1)

10. Frontier Models are Capable of In-context Scheming [Arxiv](https://arxiv.org/abs/2412.04984)

11. PaliGemma 2: A Family of Versatile VLMs for Transfer [Arxiv](https://arxiv.org/pdf/2412.03555)

12. Small Languages, Big Models: A Study of Continual Training on Languages of Norway [Arxiv](https://arxiv.org/abs/2412.06484)

13. Training Large Language Models to Reason in a Continuous Latent Space [Arxiv](https://arxiv.org/abs/2412.06769)

14. As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages [ACL](https://aclanthology.org/2021.findings-acl.74/)

15. Mixtral of experts [Arxiv](https://arxiv.org/abs/2401.04088)

16. GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities [Arxiv](https://arxiv.org/abs/2406.11768)

17. Emergent Abilities of Large Language Models [Arxiv](https://arxiv.org/abs/2206.07682)

18. Scaling laws for transfer [Arxiv](https://arxiv.org/pdf/2102.01293)